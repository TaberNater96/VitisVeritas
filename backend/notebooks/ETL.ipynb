{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae1f4f6",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">ETL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00641b83",
   "metadata": {},
   "source": [
    "## Varietals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592232d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varietals table populated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = sqlalchemy.create_engine(DATABASE_URL)\n",
    "\n",
    "# Varietals table, to add more data, first delete current data to prevent duplicates and then re-run this cell\n",
    "varietals_data = {\n",
    "    \"Pinot Noir\": \"The quintessential Willamette Valley grape. It presents a silky, elegant texture balanced by vibrant acidity. Expect a complex aromatic profile of wild strawberry, Rainier cherry, and damp forest floor, with a subtle spice of white pepper. The volcanic Jory soils often impart a distinct stony minerality, while marine sedimentary soils can add a savory, umami-like depth.\",\n",
    "    \"Pinot Gris\": \"A textural and aromatic delight, often crafted in an Alsatian-inspired style. It reveals layers of ripe pear, quince, and beeswax, with a spicy hint of ginger. A brief period of skin contact lends a beautiful copper hue and a pleasant bitter note of orange peel, adding to its complexity.\",\n",
    "    \"Chardonnay\": \"A grape of two distinct personalities in the Valley. The 'Lean' style, often from the Eola-Amity Hills, offers crisp green apple, wet flint, and lemon pith with laser-like acidity. The 'Rich' style, typical of the Dundee Hills, delivers notes of baked apple, brioche, and toasted hazelnut with a rounder, creamier mid-palate.\",\n",
    "    \"Riesling\": \"A noble grape with remarkable aging potential. In its youth, it bursts with notes of peach skin and a hint of kerosene, which evolves into complex honeycomb and petrol notes over time. Dry expressions possess a searing acidity, while late-harvest styles are luscious with apricot nectar.\",\n",
    "    \"Gamay\": \"Vibrant and juicy, this grape is all about immediate pleasure. It explodes with flavors of cranberry, pomegranate, and rose hip, grounded by a subtle black tea leaf earthiness. Carbonic maceration often lends a characteristic bubblegum lift and softens the tannins, making it irresistibly quaffable.\",\n",
    "    \"Syrah\": \"A testament to cool-climate intensity. This grape offers a brooding bouquet of blackberry compote, violets, and savory notes of smoked meat and black olive. The persistent winds of the Van Duzer Corridor impart a signature peppery grip and firm, chewy tannins.\",\n",
    "    \"Pinot Blanc\": \"A varietal of quiet power and finesse. It whispers notes of lemon curd, raw almond, and a distinct saline minerality reminiscent of sea spray. It is often blended with Chardonnay to enhance textural weight and complexity.\",\n",
    "    \"Dolcetto\": \"An Oregonian take on a Piedmont classic. It features a core of bitter cherry, interwoven with notes of tar, licorice, and blueberry skin. Compared to its Italian counterpart, the Willamette Valley version often showcases brighter acidity and a unique iron oxide grit from the local soils.\",\n",
    "    \"Tempranillo\": \"Feral, savory, and complex. This Spanish grape reveals a rustic profile of sour plum, tobacco leaf, and worn leather. It demands sun-drenched, south-facing slopes to achieve full ripeness and express its sun-baked character.\",\n",
    "    \"Lagrein\": \"Inky, brooding, and powerful. This Northern Italian varietal is characterized by intense notes of black raspberry, dark cocoa powder, and a ferrous hint of beef blood. The tannins are famously robust, often described as 'chewing on grape seeds.'\",\n",
    "    \"Carmine\": \"A fascinating hybrid with a bold personality. It combines the structure of Cabernet Sauvignon with the rustic charm of Carignan, resulting in a wine with notes of cranberry sauce, bell pepper, and dark chocolate.\",\n",
    "    \"Grüner Veltliner\": \"Austria's star grape, finding a happy home in Oregon. It is defined by its signature white pepper and radish-like crunch, complemented by flavors of green pear, lentil, and lime zest. The local Jory soils contribute a smoky, flinty nuance.\",\n",
    "    \"Müller-Thurgau\": \"A charming, off-dry white perfect for casual sipping. It delights with aromatic notes of lychee, elderflower, and bruised apple. Its naturally low alcohol content makes it a light and refreshing picnic wine.\",\n",
    "    \"Albariño\": \"A burst of seaside freshness. This Spanish varietal is defined by its saline minerality, evoking sea spray on a coastal breeze. Flavors of white peach and lemongrass are carried on a wave of crisp acidity, making it exceptionally refreshing.\",\n",
    "    \"Arneis\": \"A varietal of bitter elegance from Piedmont. It offers a sophisticated bouquet of fennel pollen, chamomile, and blanched almond. The finish is characteristically bitter, reminiscent of a peach pit, which adds to its allure.\",\n",
    "    \"Pinot Meunier\": \"Primarily a sparkling wine grape, it shines with notes of red apple skin and freshly baked brioche. It contributes to a creamy, persistent mousse, creating a wine of great texture and charm.\",\n",
    "    \"Gewürztraminer\": \"An aromatic explosion in the glass. It offers an unmistakable bouquet of lychee and rosewater, with a rich, slightly oily texture that coats the palate. Willamette Valley versions are often fermented dry, showcasing the grape's full aromatic potential.\",\n",
    "    \"Pinot Noir Précoce\": \"An early-ripening clone, considered an ancestor of Pinot Noir. It delivers a wilder, more rustic expression of its descendant, with intense notes of wild strawberry and blood orange.\",\n",
    "    \"Blaufränkisch\": \"An Austrian red with a savage edge. It presents a deep profile of blueberry liqueur and graphite, supported by fierce, structured tannins that demand attention.\",\n",
    "    \"Sauvignon Blanc\": \"A nod to the restrained style of the Loire Valley. It offers crisp notes of gooseberry and freshly cut grass, with a focus on minerality and elegance rather than overt tropical fruit.\"\n",
    "}\n",
    "\n",
    "df_varietals = pd.DataFrame(list(varietals_data.items()), columns=['name', 'description'])\n",
    "df_varietals.to_sql('varietals', engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Varietals table populated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53813504",
   "metadata": {},
   "source": [
    "## Wineries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb3e6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 AVAs into memory.\n",
      "Database connection configured.\n",
      "Loaded 140 previously ingested wineries from 'wineries_ingested.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "ava_data = {\n",
    "    \"Chehalem Mountains\": 10,\n",
    "    \"Dundee Hills\": 3,\n",
    "    \"Eola-Amity Hills\": 4,\n",
    "    \"Laurelwood District\": 11,\n",
    "    \"Lower Long Tom\": 6,\n",
    "    \"McMinnville\": 7,\n",
    "    \"Mt Pisgah Polk County\": 5,\n",
    "    \"Ribbon Ridge\": 9,\n",
    "    \"Tualatin Hills\": 12,\n",
    "    \"Van Duzer\": 2,\n",
    "    \"Willamette Valley\": 1,\n",
    "    \"Yamhill-Carlton\": 8\n",
    "}\n",
    "print(f\"Loaded {len(ava_data)} AVAs into memory.\")\n",
    "\n",
    "\n",
    "dotenv_path = '../.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "\n",
    "if not DATABASE_URL:\n",
    "    raise ValueError(f\"DATABASE_URL not found. Check your .env file path.\")\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "print(\"Database connection configured.\")\n",
    "\n",
    "\n",
    "TRACKING_CSV_PATH = 'wineries_ingested.csv'\n",
    "ingested_wineries_set = set()\n",
    "\n",
    "if os.path.exists(TRACKING_CSV_PATH):\n",
    "    df_ingested = pd.read_csv(TRACKING_CSV_PATH)\n",
    "    ingested_wineries_set = set(df_ingested['name'].tolist())\n",
    "    print(f\"Loaded {len(ingested_wineries_set)} previously ingested wineries from '{TRACKING_CSV_PATH}'.\")\n",
    "else:\n",
    "    pd.DataFrame(columns=['name']).to_csv(TRACKING_CSV_PATH, index=False)\n",
    "    print(f\"Created new tracking file: '{TRACKING_CSV_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d4608",
   "metadata": {},
   "source": [
    "#### Individual Winery Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb13b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "winery_name = \"\"\n",
    "winery_address = \"\"\n",
    "winery_website = \"\"\n",
    "winery_description = \"\"\n",
    "winery_latitude = \n",
    "winery_longitude = \n",
    "winery_ava_name = \"Eola-Amity Hills\"\n",
    "winery_phone = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09deb938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing: VinTyr ---\n",
      "Found AVA: 'Eola-Amity Hills' (ID: 4)\n",
      "✅ Successfully inserted 'VinTyr' into the database.\n",
      "✅ Updated tracking file. Total wineries ingested: 141.\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Processing: {winery_name} ---\")\n",
    "\n",
    "# Check for duplicates first\n",
    "if winery_name in ingested_wineries_set:\n",
    "    print(f\"🔴 STOP: Winery '{winery_name}' already exists in the tracking file. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        ava_id = ava_data.get(winery_ava_name)\n",
    "        \n",
    "        if ava_id is None:\n",
    "            raise ValueError(f\"AVA Name '{winery_ava_name}' not found in the dictionary. Check for typos.\")\n",
    "        \n",
    "        print(f\"Found AVA: '{winery_ava_name}' (ID: {ava_id})\")\n",
    "\n",
    "        point_wkt = f\"SRID=4326;POINT({winery_longitude} {winery_latitude})\"\n",
    "        \n",
    "        # Prepare the SQL INSERT statement\n",
    "        stmt = text(\"\"\"\n",
    "            INSERT INTO wineries (name, address, website_url, description, location, ava_id, phone)\n",
    "            VALUES (:name, :address, :website, :desc, ST_GeomFromEWKT(:loc), :ava_id, :phone)\n",
    "        \"\"\")\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            with connection.begin() as transaction:\n",
    "                connection.execute(stmt, {\n",
    "                    \"name\": winery_name,\n",
    "                    \"address\": winery_address,\n",
    "                    \"website\": winery_website,\n",
    "                    \"desc\": winery_description,\n",
    "                    \"loc\": point_wkt,\n",
    "                    \"ava_id\": ava_id,\n",
    "                    \"phone\": winery_phone\n",
    "                })\n",
    "        \n",
    "        print(f\"✅ Successfully inserted '{winery_name}' into the database.\")\n",
    "        \n",
    "        # --- 3. UPDATE TRACKING FILE ---\n",
    "        pd.DataFrame([{'name': winery_name}]).to_csv(TRACKING_CSV_PATH, mode='a', header=False, index=False)\n",
    "        ingested_wineries_set.add(winery_name)\n",
    "        print(f\"✅ Updated tracking file. Total wineries ingested: {len(ingested_wineries_set)}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🔴 ERROR: Failed to insert '{winery_name}'.\")\n",
    "        print(f\"Error details: {e}\")\n",
    "\n",
    "print(\"-\" * 30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9711cad",
   "metadata": {},
   "source": [
    "## Soils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ce6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc655bd",
   "metadata": {},
   "source": [
    "#### Manual County Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba662a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTY_NAME = \"lane\"\n",
    "COUNTY_CODE = \"or637\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebe2fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing County: Lane\n",
      "------------------------------\n",
      "Expecting SHP file at: ../data/raw/soils/lane/spatial/soilmu_a_or637.shp\n",
      "Expecting TXT file at: ../data/raw/soils/lane/tabular/mapunit.txt\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_data_path = f\"../data/raw/soils/{COUNTY_NAME}/\"\n",
    "spatial_path = os.path.join(base_data_path, \"spatial\", f\"soilmu_a_{COUNTY_CODE}.shp\")\n",
    "tabular_path = os.path.join(base_data_path, \"tabular\", \"mapunit.txt\")\n",
    "\n",
    "# Verify Paths\n",
    "print(f\"Processing County: {COUNTY_NAME.title()}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Expecting SHP file at: {spatial_path}\")\n",
    "print(f\"Expecting TXT file at: {tabular_path}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d2dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Reading spatial and tabular data...\n",
      "   ...Data extracted successfully. Found 24 columns in tabular data.\n",
      "2. Transforming and cleaning data...\n",
      "   - Cleaning join keys...\n",
      "   - Dropping conflicting columns from shapefile data: ['musym']\n",
      "   - Merging spatial and tabular data on 'mukey'...\n",
      "   - Adding county information: lane\n",
      "   - Repairing invalid geometries...\n",
      "   - Preparing data for database...\n",
      "   ...Transformation complete.\n",
      "3. Loading data into PostGIS...\n",
      "Success! The soil data has been ingested into the PostGIS database.\n",
      "Loaded 36195 soil polygons for Lane County.\n",
      "\n",
      "Data Summary:\n",
      "   - County: lane\n",
      "   - Total polygons: 36195\n",
      "   - Unique soil types (mukey): 310\n",
      "   - Sample soil names: ['Dixonville-Philomath-Hazelair complex, 3 to 12 percent slopes', 'Panther silty clay loam, 2 to 12 percent slopes', 'Nekia silty clay loam, 12 to 20 percent slopes']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # --- 1. EXTRACT ---\n",
    "    print(\"1. Reading spatial and tabular data...\")\n",
    "    gdf_spatial = gpd.read_file(spatial_path)\n",
    "    df_tabular = pd.read_csv(tabular_path, sep='|', header=None, dtype=str)\n",
    "    \n",
    "    # Dynamically create placeholder column names for the tabular data\n",
    "    num_cols = len(df_tabular.columns)\n",
    "    column_names = [f'col_{i}' for i in range(num_cols)]\n",
    "    \n",
    "    # Assign mukey to the LAST column (index -1)\n",
    "    column_names[0] = 'musym'    # Map Unit Symbol\n",
    "    column_names[1] = 'muname'   # Map Unit Name\n",
    "    column_names[-1] = 'mukey'   # Map Unit Key (THE LAST COLUMN)\n",
    "    \n",
    "    df_tabular.columns = column_names\n",
    "    print(f\"   ...Data extracted successfully. Found {num_cols} columns in tabular data.\")\n",
    "\n",
    "    # --- 2. TRANSFORM ---\n",
    "    print(\"2. Transforming and cleaning data...\")\n",
    "    \n",
    "    # Standardize column names and join keys\n",
    "    gdf_spatial.rename(columns=str.lower, inplace=True)\n",
    "    \n",
    "    # Clean join keys by removing whitespace and ensuring string type\n",
    "    print(\"   - Cleaning join keys...\")\n",
    "    gdf_spatial['mukey'] = gdf_spatial['mukey'].astype(str).str.strip()\n",
    "    df_tabular['mukey'] = df_tabular['mukey'].astype(str).str.strip()\n",
    "\n",
    "    # Prevent column name collisions\n",
    "    cols_to_add = ['musym', 'muname']\n",
    "    cols_to_drop = [col for col in cols_to_add if col in gdf_spatial.columns]\n",
    "    if cols_to_drop:\n",
    "        print(f\"   - Dropping conflicting columns from shapefile data: {cols_to_drop}\")\n",
    "        gdf_spatial.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Merge the dataframes\n",
    "    print(\"   - Merging spatial and tabular data on 'mukey'...\")\n",
    "    merged_gdf = gdf_spatial.merge(\n",
    "        df_tabular[['mukey', 'musym', 'muname']], \n",
    "        on='mukey', \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # ADD COUNTY INFORMATION - This is the key addition\n",
    "    print(f\"   - Adding county information: {COUNTY_NAME}\")\n",
    "    merged_gdf['county'] = COUNTY_NAME.lower()  # Store county name in lowercase\n",
    "\n",
    "    # Repair invalid geometries using a buffer(0) trick\n",
    "    print(\"   - Repairing invalid geometries...\")\n",
    "    merged_gdf['geometry'] = merged_gdf.buffer(0)\n",
    "\n",
    "    # Prepare final dataframe for the database - NOW INCLUDING COUNTY\n",
    "    print(\"   - Preparing data for database...\")\n",
    "    final_gdf = merged_gdf[['mukey', 'musym', 'muname', 'county', 'geometry']].copy()\n",
    "    \n",
    "    # Check for merge failures before loading\n",
    "    unmerged_count = final_gdf['muname'].isnull().sum()\n",
    "    if unmerged_count > 0:\n",
    "        print(f\"   WARNING: {unmerged_count} polygons could not be matched to tabular data.\")\n",
    "\n",
    "    final_gdf = final_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Make sure all geometries are MultiPolygons\n",
    "    final_gdf['geometry'] = [MultiPolygon([geom]) if geom.geom_type == 'Polygon' else geom for geom in final_gdf.geometry]\n",
    "    \n",
    "    # Rename the geometry column to match the database table schema ('geom')\n",
    "    final_gdf.rename(columns={'geometry': 'geom'}, inplace=True)\n",
    "    final_gdf = final_gdf.set_geometry('geom') # set the newly renamed 'geom' column as the active geometry\n",
    "    print(\"   ...Transformation complete.\")\n",
    "\n",
    "    # --- 3. LOAD ---\n",
    "    print(\"3. Loading data into PostGIS...\")\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    with engine.connect() as connection:\n",
    "        final_gdf.to_postgis(\n",
    "            name='soils',\n",
    "            con=connection,\n",
    "            if_exists='append',  # using append since we may load multiple counties\n",
    "            index=False,\n",
    "            dtype={'geom': 'GEOMETRY(MultiPolygon, 4326)'}\n",
    "        )\n",
    "    \n",
    "    print(\"Success! The soil data has been ingested into the PostGIS database.\")\n",
    "    print(f\"Loaded {len(final_gdf)} soil polygons for {COUNTY_NAME.title()} County.\")\n",
    "    \n",
    "    # Display summary of what was loaded\n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"   - County: {COUNTY_NAME}\")\n",
    "    print(f\"   - Total polygons: {len(final_gdf)}\")\n",
    "    print(f\"   - Unique soil types (mukey): {final_gdf['mukey'].nunique()}\")\n",
    "    print(f\"   - Sample soil names: {final_gdf['muname'].head(3).tolist()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An error occurred during the ETL process.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86438944",
   "metadata": {},
   "source": [
    "#### Lane County Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d465e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial file size: 116.43 MB\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import os\n",
    "\n",
    "# Check initial file size\n",
    "initial_size = os.path.getsize('lane_soils.geojson') / (1024 * 1024)\n",
    "print(f\"Initial file size: {initial_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92525915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 36195\n",
      "CRS: EPSG:4326\n",
      "Columns: ['mukey', 'musym', 'muname', 'county', 'geometry']\n",
      "Original bounds: [-124.15937568   43.53917257 -122.019946     44.29049607]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('lane_soils.geojson')\n",
    "\n",
    "print(f\"Number of features: {len(gdf)}\")\n",
    "print(f\"CRS: {gdf.crs}\")\n",
    "print(f\"Columns: {list(gdf.columns)}\")\n",
    "print(f\"Original bounds: {gdf.total_bounds}\")  # [minx, miny, maxx, maxy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1101dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after clipping: 29787\n",
      "New bounds: [-123.61722      43.53917257 -122.019946     44.29049607]\n"
     ]
    }
   ],
   "source": [
    "# Define the western boundary\n",
    "west_boundary = -123.61722\n",
    "\n",
    "# Get current bounds\n",
    "bounds = gdf.total_bounds\n",
    "\n",
    "# Create a clipping box from the western boundary to the eastern edge, extending north/south beyond the data bounds to ensure full coverage\n",
    "clip_box = box(west_boundary, bounds[1] - 1, bounds[2] + 1, bounds[3] + 1)\n",
    "\n",
    "# Convert to GeoDataFrame for clipping\n",
    "clip_gdf = gpd.GeoDataFrame([1], geometry=[clip_box], crs=gdf.crs)\n",
    "clipped_gdf = gdf.clip(clip_gdf) # perform the actual clip\n",
    "\n",
    "# Remove any empty geometries that might result from clipping\n",
    "clipped_gdf = clipped_gdf[~clipped_gdf.geometry.is_empty]\n",
    "\n",
    "print(f\"Features after clipping: {len(clipped_gdf)}\")\n",
    "print(f\"New bounds: {clipped_gdf.total_bounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a8f9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as lane_soils.geojson\n"
     ]
    }
   ],
   "source": [
    "clipped_gdf.to_file('lane_soils.geojson', driver='GeoJSON')\n",
    "print(\"File saved as lane_soils.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5504d3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file size: 116.43 MB\n",
      "New file size: 92.41 MB\n",
      "Size reduction: 20.6%\n",
      "✓ Success! File is now under 100 MB and ready for GitHub\n"
     ]
    }
   ],
   "source": [
    "# Verify the new file size\n",
    "new_size = os.path.getsize('lane_soils.geojson') / (1024 * 1024)\n",
    "reduction = ((initial_size - new_size) / initial_size) * 100\n",
    "\n",
    "print(f\"Original file size: {initial_size:.2f} MB\")\n",
    "print(f\"New file size: {new_size:.2f} MB\")\n",
    "print(f\"Size reduction: {reduction:.1f}%\")\n",
    "\n",
    "if new_size < 100:\n",
    "    print(\"✓ Success! File is now under 100 MB and ready for GitHub\")\n",
    "else:\n",
    "    print(f\"⚠ File is still {new_size - 100:.2f} MB over the limit\")\n",
    "    print(\"You may need to clip more aggressively or add geometric simplification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
